{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21b5e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação das bibliotecas necessárias para acessar o serviço Groq\n",
    "# e carregar o modelo de linguagem Llama 3.3 Versatile.\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Carregar as variáveis de ambiente do arquivo .env\n",
    "load_dotenv()\n",
    "\n",
    "# Obter a chave da API do Groq a partir das variáveis de ambiente\n",
    "# Verificar se a chave da API foi definida corretamente\n",
    "api_key = os.getenv('GROG_API_KEY')\n",
    "if not api_key:\n",
    "    raise ValueError(\"GROG_API_KEY não está definida. Por favor, defina a chave da API no arquivo .env.\")\n",
    "\n",
    "# Inicializar o modelo de linguagem Llama 3.3 Versatile usando a chave da API\n",
    "llm = ChatGroq(\n",
    "    model_name=\"llama-3.3-70b-versatile\",\n",
    "    api_key=api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf6b5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de uso do modelo de linguagem para invocar uma pergunta\n",
    "prompt = \"Qual é a capital da França?\"\n",
    "\n",
    "# Invocar o modelo de linguagem com o prompt\n",
    "response = llm.invoke(prompt)\n",
    "\n",
    "# Exibir a resposta do modelo\n",
    "print(f\"Resposta do modelo: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2ca0a3",
   "metadata": {},
   "source": [
    "### Com dados de fontes internas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e70b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL do documento PDF que será utilizado como fonte de dados\n",
    "url = 'https://raw.githubusercontent.com/allanspadini/curso-flash-rag/main/m2m_strategy_and_objectives_development.pdf'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d7dda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação do carregador de documentos PDF da biblioteca LangChain Community\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# Carregar o documento PDF a partir da URL\n",
    "# Criar uma variável chamada `loader` que utiliza o PyPDFLoader para carregar o PDF\n",
    "loader = PyPDFLoader(url)\n",
    "\n",
    "# Carregar o documento PDF de forma que tenha vários blocos, ou seja, conteúdo por página\n",
    "pages = []\n",
    "\n",
    "# Utilizar o método lazy_load do loader para carregar as páginas do PDF\n",
    "for page in loader.lazy_load():\n",
    "    pages.append(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6f18d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibir os metadados\n",
    "print(f\"{pages[0].metadata}\\n\")\n",
    "\n",
    "# Explorar o conteúdo do primeiro bloco carregado\n",
    "print(f\"Conteúdo da primeira página: {pages[0].page_content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc25e25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criação de uma base de dados vetorial para armazenar as informações do PDF, usando langchain\n",
    "# Importação do InMemoryVectorStore da biblioteca LangChain Core, que permite armazenar vetores em memória\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "# Usar o HuggingFaceEmbeddings para gerar embeddings dos textos, ou seja, tranformar o texto em vetores\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Modelo de embeddings do HuggingFace para transformar o texto em vetores, modelo aberto e gratuito\n",
    "embed_model = HuggingFaceEmbeddings(model_name=\"mixedbread-ai/mxbai-embed-large-v1\")\n",
    "# necessário instalar pip install sentence-transformers para usar HuggingFaceEmbeddings\n",
    "\n",
    "# Criar uma base de dados vetorial em memória a partir dos documentos carregados\n",
    "vector_store = InMemoryVectorStore.from_documents(pages, embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d783267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Associando a variável ao VectorStore\n",
    "docs = vector_store\n",
    "\n",
    "# Realizar uma busca de similaridade no VectorStore para encontrar documentos relevantes\n",
    "docs = vector_store.similarity_search\n",
    "\n",
    "# Realizando a busca por similaridade\n",
    "#docs = vector_store.similarity_search()\n",
    "\n",
    "# Exemplo de busca por similaridade (PDF em inglês, então a consulta também deve ser em inglês)\n",
    "docs = vector_store.similarity_search(\"Objectives Development Process\")\n",
    "\n",
    "# Mostrar os resultados da busca com k=2 para limitar o número de resultados\n",
    "docs = vector_store.similarity_search(\"Objectives Development Process\", k=2)\n",
    "\n",
    "# Exibir os resultados da busca\n",
    "for doc in docs:\n",
    "    print(f\"Conteúdo: {doc.page_content}\\n\")\n",
    "    print(f\"Metadados: {doc.metadata}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa2e3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"You're a helpful assistant that only gives answer bases on the given context. If the answer is not in the context, say \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "Answer: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f5a811",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b8d74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Use the following context to answer the question:\\n\\n{context}\\n\\nQuestion: {question}\"\n",
    ")\n",
    "\n",
    "# Chain\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a450fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação do módulo display e Markdown da biblioteca IPython para exibir resultados formatados\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b2590b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke(\"What is the Objectives Development Process?\")\n",
    "display(Markdown(f\"**Resposta:** {response}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc2b06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "def pega_contexto(query: str) -> str:\n",
    "    \"\"\" Pega o contexto baseado em uma pesquisa.\"\"\"\n",
    "    retriever = vector_store.as_retriever()\n",
    "    resultado = retriever.invoke(query)\n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bc9bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def carrega_pdf(url: str):\n",
    "    loader = PyPDFLoader(url)\n",
    "    pages = []\n",
    "    for page in loader.lazy_load():\n",
    "        pages.append(page)\n",
    "\n",
    "    vectorstore = InMemoryVectorStore.from_documents(pages, embed_model)\n",
    "    return vectorstore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7cb9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store_agriculture = carrega_pdf('https://raw.githubusercontent.com/allanspadini/curso-flash-rag/main/agriculture.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a2feb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store_dengue = carrega_pdf('https://raw.githubusercontent.com/allanspadini/curso-flash-rag/main/dengue.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c220775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pega_contexto_agriculture(query: str) -> str:\n",
    "    \"\"\"Pega o contexto sobre agricultura baseado em uma pesquisa.\"\"\"\n",
    "    retriever = vector_store_agriculture.as_retriever()\n",
    "    resultado = retriever.invoke(query)\n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80d64c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pega_contexto_dengue(query: str) -> str:\n",
    "    \"\"\"Pega o contexto sobre dengue baseado em uma pesquisa.\"\"\"\n",
    "    retriever = vector_store_dengue.as_retriever()\n",
    "    resultado = retriever.invoke(query)\n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fe5ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [pega_contexto, pega_contexto_agriculture, pega_contexto_dengue]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b45daaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pega_contexto_dengue(\"Cases of dengue we had since the beginning of 2025?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e19c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e806e8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria o agente sem system_prompt\n",
    "agente_pdf = create_react_agent(\n",
    "    model=llm,  # seu LLM\n",
    "    tools=tools  # suas ferramentas\n",
    ")\n",
    "\n",
    "# Se quiser definir o system prompt, use invoke com o texto de instrução no primeiro \"message\"\n",
    "resultado = agente_pdf.invoke({\n",
    "    \"messages\": [\n",
    "        [\"system\", \"\"\"You're a helpful assistant that only gives answers based on the given context. \n",
    "        If the answer is not in the context, say \"I don't know\".\n",
    "        - pega_contexto: Tool that returns the context based on the users query if the query is about NASA and space travels.\n",
    "        - pega_contexto_agriculture: Tool that returns the context based on the users query if the query is about agriculture.\n",
    "        - pega_contexto_dengue: Tool that returns the context based on the users query if the query is about dengue.\"\"\"],\n",
    "        [\"user\", \"What causes dengue?\"]\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc216b5b",
   "metadata": {},
   "source": [
    "### Adicionando memóeria ao Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f05688a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import MessagesState\n",
    "from langgraph.graph import START, StateGraph, END\n",
    "from langgraph.prebuilt import tools_condition\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from IPython.display import Image, display\n",
    "from langchain_core.messages import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98157f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grafo inicial\n",
    "grafo = StateGraph(MessagesState)\n",
    "grafo.add_node(\"assistente\", agente_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4526e44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicionando ferramentas ao grafo\n",
    "grafo.add_node(\"tools\", ToolNode(tools))\n",
    "grafo.add_edge(START, \"assistente\")\n",
    "\n",
    "# Definir conexões e condições\n",
    "grafo.add_conditional_edges(\"assistente\", tools_condition)\n",
    "grafo.add_edge(\"tools\", \"assistente\")\n",
    "grafo.add_edge(\"assistente\", END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72778050",
   "metadata": {},
   "outputs": [],
   "source": [
    "memoria = MemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6085cecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilar o grafo com o checkpointer de memória\n",
    "app = grafo.compile(checkpointer=memoria)\n",
    "\n",
    "Image(app.get_graph().draw_mermaid_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe13fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando chatbot com memória\n",
    "def chat_com_memoria(mensagem_usuario: str, thread_id=\"1\", verbose=False):\n",
    "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "    messages = app.invoke({\"messages\": [HumanMessage(content=mensagem_usuario)]}, config)\n",
    "    if verbose:\n",
    "        for message in messages['messages']:\n",
    "            message.pretty_print()\n",
    "    else:\n",
    "        messages['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b441d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste\n",
    "chat_com_memoria(mensagem_usuario=\"What is the planet NASA is going?\", thread_id=\"2\", verbose=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
